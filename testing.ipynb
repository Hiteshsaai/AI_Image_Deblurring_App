{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.utils import save_image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-e EPOCHS]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/hiteshsaaimanancherypanneerselvam/Library/Jupyter/runtime/kernel-75148bfc-3521-40a4-b1a9-a8483cb073c4.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "# constructing the argument parser\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('-e', '--epochs', type=int, default=40, \n",
    "            help='number of epochs to train the model for')\n",
    "\n",
    "args = vars(parser.parse_args())\n",
    "\n",
    "def save_decoded_image(img, name):\n",
    "    img = img.view(img.size(0), 3, 224, 224)\n",
    "    save_image(img, name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# helper functions\n",
    "image_dir = '../outputs/saved_images'\n",
    "\n",
    "os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(device)\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accumulating the images for train and test\n",
    "\n",
    "gauss_blur = os.listdir('input_images/gaussian_blurred')\n",
    "\n",
    "# sorting the folder to align with sharp images folder\n",
    "gauss_blur.sort()\n",
    "\n",
    "sharp = os.listdir('input_images/sharp')\n",
    "\n",
    "#sorting the folder to align with gaussian_blurred images folder\n",
    "sharp.sort()\n",
    "\n",
    "x_blur = []\n",
    "for i in range(len(gauss_blur)):\n",
    "    x_blur.append(gauss_blur[i])\n",
    "\n",
    "y_sharp = []\n",
    "for i in range(len(sharp)):\n",
    "    y_sharp.append(sharp[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data instances: 280\n",
      "Validation data instances: 70\n"
     ]
    }
   ],
   "source": [
    "# Train Test Split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_blur, y_sharp, test_size=0.20)\n",
    "\n",
    "print(f\"Train data instances: {len(x_train)}\")\n",
    "print(f\"Validation data instances: {len(x_val)}\")\n",
    "\n",
    "# define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(), #  Transforming the image to PIL,\n",
    "    transforms.Resize((224, 224)), # Transoforing the image to 224X224 dimension,\n",
    "    transforms.ToTensor(), #Transforming the image to torch tensor,\n",
    "])\n",
    "\n",
    "class DeblurDataset(Dataset):\n",
    "    def __init__(self, blur_paths, sharp_paths=None, transforms=None):\n",
    "        self.X = blur_paths\n",
    "        self.y = sharp_paths\n",
    "        self.transforms = transforms\n",
    "         \n",
    "    def __len__(self):\n",
    "        return (len(self.X))\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        blur_image = cv2.imread(f\"../input/gaussian_blurred/{self.X[i]}\")\n",
    "        \n",
    "        if self.transforms:\n",
    "            blur_image = self.transforms(blur_image)\n",
    "            \n",
    "        if self.y is not None:\n",
    "            sharp_image = cv2.imread(f\"../input/sharp/{self.y[i]}\")\n",
    "            sharp_image = self.transforms(sharp_image)\n",
    "            return (blur_image, sharp_image)\n",
    "        else:\n",
    "            return blur_image\n",
    "\n",
    "train_data = DeblurDataset(x_train, y_train, transform)\n",
    "val_data = DeblurDataset(x_val, y_val, transform)\n",
    " \n",
    "trainloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeblurCNN(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(9, 9), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class DeblurCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeblurCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=9, padding=2)\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=1, padding=2)\n",
    "        self.conv3 = nn.Conv2d(32, 3, kernel_size=5, padding=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n",
    "model = DeblurCNN().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( \n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        patience=5,\n",
    "        factor=0.5,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "def fit(model, dataloader, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in tqdm(enumerate(dataloader), total=int(len(train_data)/dataloader.batch_size)):\n",
    "        blur_image = data[0]\n",
    "        sharp_image = data[1]\n",
    "        blur_image = blur_image.to(device)\n",
    "        sharp_image = sharp_image.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(blur_image)\n",
    "        loss = criterion(outputs, sharp_image)\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    train_loss = running_loss/len(dataloader.dataset)\n",
    "    print(f\"Train Loss: {train_loss:.5f}\")\n",
    "    \n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-7986cc57890c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_epoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'epoch' is not defined"
     ]
    }
   ],
   "source": [
    "train_epoch_loss = fit(model, trainloader, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-6e282ca5fbeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "args['epochs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (blur_image)",
   "language": "python",
   "name": "blur_image"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
